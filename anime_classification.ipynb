{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anime-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_r9WWAsUnME",
        "colab_type": "text"
      },
      "source": [
        "# Classifying 90+ Anime Characters with Transfer Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3q-Asqm6wBV",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqITqJpwo3qd",
        "colab_type": "text"
      },
      "source": [
        "First, install the prerequisite libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTdmmn3TJZIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import zipfile"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vjawztquI4P",
        "colab_type": "text"
      },
      "source": [
        "Extract the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yScHl4c1qrPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = '/content/anime_dataset'\n",
        "\n",
        "with zipfile.ZipFile(\"/content/drive/My Drive/dataset_cropped.zip\") as datazip:\n",
        "  datazip.extractall(base_dir)\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'cropped_images', 'train')\n",
        "train_dir = os.path.join(base_dir, 'cropped_images', 'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ovNz1Z8YM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = '/content/anime_dataset'\n",
        "train_dir = os.path.join(base_dir, 'cropped_images', 'train')\n",
        "validation_dir = os.path.join(base_dir, 'cropped_images', 'validation')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxyZRXh6vJMw",
        "colab_type": "text"
      },
      "source": [
        "Setup data augmentation/batch sizes for input:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcOV8wLKunyA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1f56d8a9-01d4-4d0d-dfd0-923001c68f8c"
      },
      "source": [
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.inception_v3.preprocess_input, # Preprocess images for Inception v3\n",
        "    rotation_range=30, # Randomly rotate the image up to 30 degrees\n",
        "    horizontal_flip=True # Randomly flip the image horizontally\n",
        ")\n",
        "\n",
        "# No need for data augmentation in validation\n",
        "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_v3.preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    class_mode='categorical', # Since we're classifying many different classes \n",
        "    batch_size=32, # Flow in batches of 32 images\n",
        "    target_size=(299,299) # Reshape images to max image dimensions for Inceptionv3\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    target_size=(299,299)\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6341 images belonging to 94 classes.\n",
            "Found 1063 images belonging to 94 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy8tjB1N3D0b",
        "colab_type": "text"
      },
      "source": [
        "Import and configure the base model (Inception V3):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W8CVs3P3KZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a6cfb20d-932f-4d72-a33b-2afa5b1f2741"
      },
      "source": [
        "base_model = tf.keras.applications.InceptionV3(\n",
        "    include_top=False, # Don't include last output layer for transfer learning\n",
        "    input_shape=(299,299,3),\n",
        "    weights='imagenet' # Import the weights for imagenet\n",
        ")\n",
        "\n",
        "base_model.trainable = False # Freeze the layers so we're not retraining"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXrPC4LZ6y5a",
        "colab_type": "text"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3dGx80S4f2m",
        "colab_type": "text"
      },
      "source": [
        "Create our own layers:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAMVNjEu4kId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pool the 3D output into a 2D feature vector to connect conv layers to our fully connected layers\n",
        "global_avg_layer = tf.keras.layers.GlobalAveragePooling2D() \n",
        "fc_layer = tf.keras.layers.Dense(1024, activation='relu')\n",
        "output_layer = tf.keras.layers.Dense(94, activation='softmax') # Since we have 94 classes"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nejr_TWR66ac",
        "colab_type": "text"
      },
      "source": [
        "Build our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcQurBSP68pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.Input(shape=(299,299,3))\n",
        "x = base_model(inputs, training=False) # We need training=False since we have batch normalization layers in Inception v3\n",
        "x = global_avg_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x) # For regularization\n",
        "x = fc_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = output_layer(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swyhhrdk-DL6",
        "colab_type": "text"
      },
      "source": [
        "Compile our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PUPpvCP9_9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "b8603504-fea2-42f0-b615-10e3158af0de"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 94)                96350     \n",
            "=================================================================\n",
            "Total params: 23,997,310\n",
            "Trainable params: 2,194,526\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twmCBJpi-llV",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v96VmhUBOui",
        "colab_type": "text"
      },
      "source": [
        "Train the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuAoO6IuCC6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import PngImagePlugin\n",
        "LARGE_ENOUGH_NUMBER = 100\n",
        "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0_1Hzy3-Xtm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "outputId": "1437b22d-d4f2-4794-b05c-62100c511fc0"
      },
      "source": [
        "history = model.fit(\n",
        "    x=train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=100,\n",
        "    steps_per_epoch=198 # Since we have 6341 training images and batch sizes of 32, after 198 batches we'll have gone through a little less than the whole training set\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "198/198 [==============================] - 201s 1s/step - loss: 4.1059 - accuracy: 0.0910 - val_loss: 3.8914 - val_accuracy: 0.1656\n",
            "Epoch 2/100\n",
            "198/198 [==============================] - 199s 1s/step - loss: 3.7660 - accuracy: 0.1506 - val_loss: 3.6341 - val_accuracy: 0.1957\n",
            "Epoch 3/100\n",
            "198/198 [==============================] - 201s 1s/step - loss: 3.4918 - accuracy: 0.1973 - val_loss: 3.4116 - val_accuracy: 0.2342\n",
            "Epoch 4/100\n",
            "198/198 [==============================] - 201s 1s/step - loss: 3.2931 - accuracy: 0.2284 - val_loss: 3.2450 - val_accuracy: 0.2756\n",
            "Epoch 5/100\n",
            "198/198 [==============================] - 202s 1s/step - loss: 3.1048 - accuracy: 0.2634 - val_loss: 3.0919 - val_accuracy: 0.2926\n",
            "Epoch 6/100\n",
            "198/198 [==============================] - 202s 1s/step - loss: 2.9300 - accuracy: 0.2994 - val_loss: 2.9872 - val_accuracy: 0.3039\n",
            "Epoch 7/100\n",
            "198/198 [==============================] - 199s 1s/step - loss: 2.8119 - accuracy: 0.3143 - val_loss: 2.8762 - val_accuracy: 0.3387\n",
            "Epoch 8/100\n",
            "198/198 [==============================] - 195s 984ms/step - loss: 2.7212 - accuracy: 0.3286 - val_loss: 2.8105 - val_accuracy: 0.3537\n",
            "Epoch 9/100\n",
            "198/198 [==============================] - 200s 1s/step - loss: 2.6052 - accuracy: 0.3611 - val_loss: 2.7407 - val_accuracy: 0.3594\n",
            "Epoch 10/100\n",
            "198/198 [==============================] - 201s 1s/step - loss: 2.5444 - accuracy: 0.3685 - val_loss: 2.6829 - val_accuracy: 0.3678\n",
            "Epoch 11/100\n",
            "198/198 [==============================] - 200s 1s/step - loss: 2.4587 - accuracy: 0.3853 - val_loss: 2.6381 - val_accuracy: 0.3716\n",
            "Epoch 12/100\n",
            "198/198 [==============================] - 200s 1s/step - loss: 2.3871 - accuracy: 0.3967 - val_loss: 2.5840 - val_accuracy: 0.3829\n",
            "Epoch 13/100\n",
            "198/198 [==============================] - 201s 1s/step - loss: 2.3242 - accuracy: 0.4131 - val_loss: 2.5240 - val_accuracy: 0.3885\n",
            "Epoch 14/100\n",
            "198/198 [==============================] - 202s 1s/step - loss: 2.2693 - accuracy: 0.4286 - val_loss: 2.5026 - val_accuracy: 0.3848\n",
            "Epoch 15/100\n",
            "198/198 [==============================] - 202s 1s/step - loss: 2.2215 - accuracy: 0.4318 - val_loss: 2.4514 - val_accuracy: 0.4045\n",
            "Epoch 16/100\n",
            "198/198 [==============================] - 203s 1s/step - loss: 2.1664 - accuracy: 0.4522 - val_loss: 2.4421 - val_accuracy: 0.4045\n",
            "Epoch 17/100\n",
            "198/198 [==============================] - 203s 1s/step - loss: 2.1206 - accuracy: 0.4546 - val_loss: 2.4275 - val_accuracy: 0.3923\n",
            "Epoch 18/100\n",
            "198/198 [==============================] - 204s 1s/step - loss: 2.0872 - accuracy: 0.4620 - val_loss: 2.4011 - val_accuracy: 0.4177\n",
            "Epoch 19/100\n",
            "198/198 [==============================] - 203s 1s/step - loss: 2.0548 - accuracy: 0.4668 - val_loss: 2.3548 - val_accuracy: 0.4158\n",
            "Epoch 20/100\n",
            "147/198 [=====================>........] - ETA: 48s - loss: 2.0273 - accuracy: 0.4774"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQeZcPUNAp2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}